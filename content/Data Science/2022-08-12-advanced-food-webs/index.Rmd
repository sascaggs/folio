---
title: 'Advanced food webs'
author: Shane A. Scaggs
date: '2022-08-12'
slug: advanced-food-webs
categories:
  - Networks
tags:
  - ecology
  - simulation
  - theory
  - food webs
draft: yes
bibliography: fw_adv.bib
csl: pnas.csl
---

```{r, echo=F, message=F, warning=F}
library(tidyverse)
library(tidygraph)
library(igraph)
library(ggraph)
library(patchwork)

gtheme = theme(panel.background = element_rect(fill='white', color='white'), 
                plot.background = element_blank(), 
                title = element_text(hjust = 10))

ptheme = theme_classic() + theme(panel.background = element_rect(fill='white', color='white'), 
                                plot.background = element_blank(), 
                                title = element_text(hjust = 10))

source("~/Shane's Projects/visualization tools/color palettes.R")

```

Welcome to the second post focusing on simulating food webs. We left off with a random food web function that we used to look at how different proportions of basal species can influence the structure of a random food web. Check that post out [here](https://shanescaggs.netlify.app/data-science/basic-properties-of-food-webs/).   

> There is a problem though. Empirical food webs aren't so random. 

Food webs are  *nested* in the sense that the diets of some species tend to be subsets of the diets of another species. What this means is that specialist species tend to be eaten by other more generalist species, who are in turn eaten by other, even more generalist species. This nestedness phenomenon can arise because of allometric scaling [@petchey2008size]: a species tends to consume resources that are smaller in size than they are. This implies that the larger a species is, the more general it *could* be [@nordstrom2015nestedness], although this is not a hard rule. 

Food webs also exhibit *modularity* -- "a tendency for nodes to cluster" [@gilarranz2017effects]. Across many kinds of networks, modularity is believed to be a property that makes a network more resilient to disturbances. This is because having modules limits the propagation of disturbances through the network architecture. A related concept is echo chamber. 

# Nestedness 

A relatively simple way to added nestedness to our random simulation would be to weight the probability of each edge based on the difference between the body masses of the two species. To do this, I set up the the following functions: 

```{r}
inv_logit = function(x) exp(x)/(1+exp(x))

fwfun = function(N = 40, 
                 p_basal = 0.6, 
                 mean = 0, 
                 sd_seq = 1:3,
                 seed = 777 ) {
    
    # setup
    #set.seed(seed)
    V = list()
    G = list()
    
    # create grid and assign basal species
    e0 = expand.grid(1:N, 1:N)
    e0$basal = e0$Var1 <= (N * p_basal)
    e0$did = 1:nrow(e0)
    
    # create vertex attributes
    # WARNINGS 
    if(any(sd_seq <= 0) == T) { 
        print('Standard Deviations cannot be negative or zero.') } 
    else if(length(sd_seq) < 1) { 
        print("Vector of standard deviations (i.e., sd_seq) cannot be zero.") } 
    
    else { 
        for(i in seq_along(sd_seq)) {
            v = tibble(
                name = 1:N, 
                mass = rnorm(N, mean = mean, sd = i), 
                basal = name <= (N * p_basal)
            ) 
            V[[i]] <- v
        }
    }
    
    for(k in seq_along(sd_seq)) {
        
        # create edgelist with body mass values
        e1 = merge(e0, V[[k]][,1:2], by.x = 'Var1', by.y = 'name')
        e2 = merge(e1, V[[k]][,1:2], by.x = 'Var2', by.y = 'name')
        
        # calculate mass 
        edgelist = suppressMessages(left_join(e1, e2)) %>% 
            select(Var1, Var2, did, basal, mass.x, mass.y) %>%
            arrange(did) %>%
            mutate(diff = mass.x - mass.y, 
                   diff_prob = inv_logit(diff), 
                   ij = rbinom(n(), size=1, prob = diff_prob), 
                   ij = ifelse(basal == T, 0, ij)) %>%
            filter(ij == 1)
        
        # make graphs 
        G[[k]] = graph.data.frame( edgelist, vertices = V[[k]] ) 
        
    }
    return(G)
}
```

This function -- `fwfun` -- receives the number of species, `N`; the proportion of basal species, `p_basal`; the `mean` body mass of all species; and a sequence of standard deviations, `sd_seq`. These parameters are used to do the following: 

1. Create a grid of all pairs of species, with basal species tagged.
2. Create a vertex attribute table with variables `name`, `basal`, and `mass`. One table is generated for the supplied `mean` and each value in the `sd_seq`. The length of `sd_seq` determines the number of networks that are generated by the function.  
3. The vertex attributes are merged with the grid and then the difference between the body masses is calculated.
4. The difference is then transformed into a probability using `inv_logit` and this probability is supplied to the `rbinom` function to generate the presence/absence of edges in the food web. 
5. Finally, basal species are given a `0` and then all `0` edges are filtered out before the edgelist is used to create an `igraph` object with `graph.data.frame`. 



The most important parts of this function are the `rnorm` distribution and the calculated of the mass differences. It is worth exploring which distribution makes sense for body mass because the specifics of this distribution will have consequences for the food web topology [@thierry2011consequences].

